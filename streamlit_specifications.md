
# Streamlit Application Specification: Patient Cohort Discovery

## 1. Application Overview

The "Patient Cohort Discovery" Streamlit application aims to empower Healthcare Administrators to uncover hidden patterns and natural groupings within de-identified patient data. By leveraging unsupervised machine learning techniques such as K-Means and Hierarchical Clustering, alongside dimension reduction methods like PCA and t-SNE, the application facilitates the identification of distinct patient cohorts. The ultimate goal is to enable data-driven decision-making for personalized intervention strategies, optimizing patient care and resource allocation.

**Learning Goals:**
*   Understand the process of preparing raw patient data for machine learning.
*   Explore high-dimensional patient data through dimension reduction visualizations.
*   Apply and compare different clustering algorithms (K-Means, Hierarchical Clustering) to identify patient cohorts.
*   Evaluate clustering quality using metrics like the Elbow Method, Silhouette Score, and Adjusted Rand Index.
*   Characterize identified cohorts by their defining features.
*   Generate actionable reports to inform personalized healthcare strategies.

## 2. User Interface Requirements

The application will be structured as a multi-step workflow, guiding the Healthcare Administrator through data preparation, analysis, and reporting.

### Layout and Navigation Structure

The application will feature a sidebar for global controls and navigation, and a main content area organized into logical, sequential sections using tabs or `st.expander` components to manage complexity.

*   **Sidebar (`st.sidebar`):**
    *   **Data Source Selection:** Radio buttons to choose between "Generate Synthetic Data" and "Upload Patient Data (CSV)".
    *   **Synthetic Data Generation Controls:** (Conditionally displayed if "Generate Synthetic Data" is selected)
        *   `n_samples` (number input): Default to 800.
        *   `random_state` (number input): Default to 42.
    *   **Data Upload Control:** (Conditionally displayed if "Upload Patient Data (CSV)" is selected)
        *   `st.file_uploader` for CSV file.
    *   **Clustering Parameters:**
        *   **Number of Clusters ($k$)**: `st.slider` (range: 2-10, default based on optimal k from evaluation).
        *   **Clustering Algorithm Selection**: Radio buttons: "K-Means", "Hierarchical Clustering".
        *   **(Optional for Hierarchical)**: Dropdown for `linkage` method (e.g., 'ward', 'average', 'complete', 'single' - default 'ward').
    *   **Dimension Reduction Parameters:**
        *   **Method Selection**: Radio buttons: "PCA", "t-SNE".
        *   **Number of Components**: `st.slider` (range: 2-3, default 2).
        *   **(Optional for t-SNE)**: `perplexity` (number input/slider, default 30), `n_iter` (number input, default 1000).

*   **Main Content Area:** Divided into sequential sections, possibly using `st.tabs` or `st.expander` for a structured flow:
    1.  **Introduction & Setup:** Display initial markdown and dependencies (conceptual).
    2.  **Data Exploration:** Display data summary and initial visualizations.
    3.  **Data Preprocessing:** Explain steps, show processed data snippet.
    4.  **Dimension Reduction:** Visualize data in 2D/3D.
    5.  **Clustering (K-Means):** Optimal $k$ evaluation and cluster visualization.
    6.  **Clustering (Hierarchical):** Dendrogram and cluster visualization.
    7.  **Evaluation & Comparison:** Display metrics and individual Silhouette plot.
    8.  **Cohort Characterization:** Profile charts for numerical and categorical features.
    9.  **Patient Cohort Report:** Final actionable report.

### Input Widgets and Controls

*   **Data Input:**
    *   `st.radio('Choose Data Source', ['Generate Synthetic Data', 'Upload Patient Data'])`
    *   `st.number_input('Number of Synthetic Samples', min_value=100, max_value=2000, value=800, key='n_samples_input')`
    *   `st.number_input('Random State for Synthetic Data', value=42, key='random_state_input')`
    *   `st.file_uploader('Upload CSV File', type=['csv'], key='csv_uploader')`
*   **Clustering Parameters:**
    *   `st.slider('Number of Clusters (k)', min_value=2, max_value=10, value=4, key='num_clusters_input')` (Default value should be dynamic, based on optimal $k$ if determined, otherwise a reasonable starting point like 4-5).
    *   `st.radio('Clustering Algorithm', ['K-Means', 'Hierarchical Clustering'], key='clustering_algo_input')`
    *   `st.selectbox('Hierarchical Linkage Method', ['ward', 'average', 'complete', 'single'], index=0, key='linkage_method_input')` (Conditional)
*   **Dimension Reduction Parameters:**
    *   `st.radio('Dimension Reduction Method', ['PCA', 't-SNE'], key='dim_red_method_input')`
    *   `st.slider('Number of Components', min_value=2, max_value=3, value=2, key='n_components_input')`
    *   `st.slider('t-SNE Perplexity', min_value=5, max_value=50, value=30, key='perplexity_input')` (Conditional for t-SNE)
    *   `st.number_input('t-SNE Number of Iterations', min_value=250, max_value=2000, value=1000, key='n_iter_input')` (Conditional for t-SNE)
*   **Action Buttons:**
    *   `st.button('Generate Data / Upload Data', key='data_button')`
    *   `st.button('Perform Preprocessing', key='preprocess_button')`
    *   `st.button('Run Dimension Reduction', key='dim_red_button')`
    *   `st.button('Run K-Means Clustering', key='kmeans_button')`
    *   `st.button('Run Hierarchical Clustering', key='hac_button')`
    *   `st.button('Evaluate & Compare Clusters', key='eval_button')`
    *   `st.button('Characterize Cohorts', key='characterize_button')`
    *   `st.button('Generate Final Report', key='report_button')`

### Visualization Components (Charts, Graphs, Tables)

*   **Data Exploration:**
    *   `st.dataframe()`: Display `patient_data.head()` and `patient_data.describe(include='all')`.
    *   `st.pyplot()`:
        *   Histograms for numerical features: 'age', 'medical_history_score', 'num_diagnoses', 'num_prescriptions', 'visit_frequency_per_year'.
        *   Count plots for categorical features: 'gender', 'insurance_type', 'chronic_condition_A_present', 'chronic_condition_B_present'.
*   **Dimension Reduction:**
    *   `st.pyplot()`:
        *   2D/3D Scatter plots of patient data projected by PCA, unclustered.
        *   2D/3D Scatter plots of patient data projected by t-SNE, unclustered.
        *   2D/3D Scatter plots of patient data projected by PCA, with points colored by discovered cluster labels (K-Means and Hierarchical).
        *   2D/3D Scatter plots of patient data projected by t-SNE, with points colored by discovered cluster labels (K-Means and Hierarchical).
*   **Clustering Evaluation:**
    *   `st.pyplot()`:
        *   Elbow Method plot (SSE vs. $k$).
        *   Silhouette Scores plot (score vs. $k$).
        *   Dendrogram for hierarchical clustering, with a cut line corresponding to the chosen $k$.
        *   Individual Silhouette Scores plot for the chosen primary clustering method (e.g., K-Means).
*   **Cohort Characterization:**
    *   `st.pyplot()`:
        *   Bar plots for numerical features (mean values per cohort).
        *   Stacked bar plots for categorical features (proportions per cohort).

### Interactive Elements and Feedback Mechanisms

*   **Progress Indicators:** `st.spinner` or `st.progress` for long-running computations (e.g., t-SNE, clustering).
*   **Informative Text:** `st.markdown` to provide explanations, interpretations, and context at each step, tailored for the Healthcare Administrator persona.
*   **Metric Display:** `st.info` or `st.metric` to display calculated values like Silhouette Scores, ARI, and PCA explained variance ratio.
*   **Dynamic Visualizations:** Plots will update dynamically based on user selections (e.g., number of clusters, dimension reduction method).
*   **Cluster Size Display:** `st.dataframe` or `st.write` to show `value_counts()` of clusters.
*   **Report Generation:** A final section presenting the "Patient Cohort Report" in `st.markdown` format.

## 3. Additional Requirements

*   **Annotation and Tooltip Specifications:**
    *   All plots generated via `matplotlib.pyplot` will include clear titles, axis labels, and legends as demonstrated in the notebook.
    *   For plots with clusters, the legend will clearly map colors to cluster IDs.
    *   Key metrics (e.g., optimal $k$, average Silhouette Score) will be annotated on plots where relevant (e.g., red dashed line on Silhouette plot).
    *   `st.markdown` will be used to add textual annotations and explanations directly above or below plots, guiding the user's interpretation.
    *   For interactive plots (if using libraries like Plotly or Altair), hover tooltips will display detailed information (e.g., patient ID, original feature values, cluster assignment). For now, `matplotlib` static plots will be used.
*   **Save the states of the fields properly so that changes are not lost:**
    *   All user inputs (e.g., number of samples, random state, uploaded file, number of clusters, selected algorithms, dimension reduction parameters) will be stored in `st.session_state`.
    *   Intermediate computation results (e.g., original `patient_data` DataFrame, `patient_processed_df`, `pca_reduced_data`, `tsne_reduced_data`, `kmeans_labels`, `hierarchical_labels`) will also be saved in `st.session_state` to prevent re-computation upon widget interaction and ensure a smooth user experience through the workflow steps.

## 4. Notebook Content and Code Requirements

All markdown and code from the Jupyter Notebook will be systematically extracted and integrated into the Streamlit application. Functions will be created or adapted from the notebook's code cells, and markdown text will be displayed using `st.markdown`.

---

### **Section 1: Application Title & Introduction**

**Markdown Content:**
```markdown
# Patient Cohort Discovery for Personalized Intervention

# Patient Cohort Discovery for Personalized Intervention: A Healthcare Administrator's Workflow

## 1. Setting Up the Environment: Required Libraries Installation

As a Healthcare Administrator, I often need advanced analytical tools to understand complex patient data. Before I can begin analyzing our patient population, I need to ensure all the necessary software libraries are installed. These libraries provide the mathematical and visualization capabilities required for cohort discovery.
```
**Streamlit Implementation:**
*   Display `st.title('Patient Cohort Discovery for Personalized Intervention')`
*   Display `st.markdown` for the subsequent markdown content.
*   The `!pip install` command will not be run in the Streamlit app; libraries will be pre-installed in the environment.

### **Section 2: Importing Dependencies**

**Markdown Content:**
```markdown
## 2. Importing Dependencies

With the libraries installed, the next crucial step is to import them into my analysis environment. This makes all the functions and classes from these libraries available for use in our patient cohort discovery workflow.
```
**Code Stubs (for `imports.py` or similar):**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, adjusted_rand_score, silhouette_samples
from scipy.cluster.hierarchy import dendrogram, linkage
# from IPython.display import display, Markdown # Not needed for Streamlit

# Set aesthetic parameters for plots (Streamlit default figure size might vary, adjust for clarity)
sns.set_style("whitegrid")
# plt.rcParams['figure.figsize'] = (10, 6) # Configure for Streamlit
# plt.rcParams['font.size'] = 12 # Configure for Streamlit
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and description.
*   The imports will be handled at the top of the `app.py` script.

### **Section 3: Simulating Patient Data and Initial Exploration**

**Markdown Content:**
```markdown
## 3. Simulating Patient Data and Initial Exploration

As a Healthcare Administrator, my first task is to get a clear picture of the patient data we're working with. Since real patient data is highly sensitive, I'll simulate a de-identified dataset that reflects the complexity and diversity of our actual patient population. This synthetic dataset will allow me to explore potential hidden patterns and understand the characteristics of different patient groups without compromising privacy. This step is crucial for identifying areas where personalized interventions could be beneficial.
```
**Code Stubs (for `data_generation.py` or `utils.py`):**
```python
def generate_synthetic_patient_data(n_samples=750, random_state=42):
    """
    Generates a synthetic dataset of de-identified patient data with underlying clusters.
    ... (rest of the function as in notebook) ...
    """
    np.random.seed(random_state)
    data = {
        'patient_id': range(1, n_samples + 1),
        'age': np.random.normal(loc=55, scale=15, size=n_samples).astype(int).clip(20, 90),
        'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples, p=[0.48, 0.49, 0.03]),
        'medical_history_score': np.random.normal(loc=5, scale=2, size=n_samples).clip(1, 10),
        'num_diagnoses': np.random.poisson(lam=3, size=n_samples).clip(1, 10),
        'num_prescriptions': np.random.poisson(lam=5, size=n_samples).clip(0, 15),
        'visit_frequency_per_year': np.random.poisson(lam=4, size=n_samples).clip(1, 12),
        'insurance_type': np.random.choice(['Private', 'Public', 'None'], n_samples, p=[0.6, 0.35, 0.05]),
        'chronic_condition_A_present': np.random.randint(0, 2, n_samples),
        'chronic_condition_B_present': np.random.randint(0, 2, n_samples),
    }
    df = pd.DataFrame(data)

    # Introduce some subtle clustering structure for demonstration
    mask1 = df['patient_id'] % 3 == 0
    count1 = mask1.sum()
    df.loc[mask1, 'age'] = np.random.normal(loc=35, scale=7, size=count1).astype(int).clip(20, 90)
    df.loc[mask1, 'medical_history_score'] = np.random.normal(loc=3, scale=1.5, size=count1).clip(1, 10)
    df.loc[mask1, 'insurance_type'] = np.random.choice(['Public', 'Private'], size=count1, p=[0.7, 0.3])
    df.loc[mask1, 'chronic_condition_A_present'] = 1

    mask2 = df['patient_id'] % 3 == 1
    count2 = mask2.sum()
    df.loc[mask2, 'age'] = np.random.normal(loc=70, scale=8, size=count2).astype(int).clip(20, 90)
    df.loc[mask2, 'visit_frequency_per_year'] = np.random.poisson(lam=7, size=count2).clip(1, 12)
    df.loc[mask2, 'num_prescriptions'] = np.random.poisson(lam=8, size=count2).clip(0, 15)
    df.loc[mask2, 'insurance_type'] = np.random.choice(['Private', 'Public'], size=count2, p=[0.8, 0.2])

    mask3 = df['patient_id'] % 3 == 2
    count3 = mask3.sum()
    df.loc[mask3, 'age'] = np.random.normal(loc=50, scale=10, size=count3).astype(int).clip(20, 90)
    df.loc[mask3, 'chronic_condition_B_present'] = 1
    return df
```
**Streamlit Implementation:**
*   **Data Generation/Upload:** Based on sidebar selection, call `generate_synthetic_patient_data` with parameters from `st.session_state` or load the uploaded CSV into `st.session_state.patient_data`.
*   `st.subheader("First 5 rows of the patient data:")`
*   `st.dataframe(st.session_state.patient_data.head())`
*   `st.subheader("Basic statistics of the patient data:")`
*   `st.dataframe(st.session_state.patient_data.describe(include='all'))`
*   **Visualizations:**
    *   Create `fig, axes` for histograms (`sns.histplot`) and count plots (`sns.countplot`).
    *   `st.pyplot(fig)` for each set of plots (numerical, then categorical).
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The initial data generation and visualization give me a foundational understanding of our patient demographics and health metrics. I can see the range of ages, the distribution of medical history scores, and the prevalence of different chronic conditions. This early exploration helps me identify potential biases or areas of high variability that might influence our cohort discovery, ensuring the subsequent steps are well-informed. For instance, if certain age groups are overrepresented, I should consider how that might affect clustering.
    ```
    *   `st.markdown` for the explanation.

### **Section 4: Data Preprocessing for Clustering Algorithms**

**Markdown Content:**
```markdown
## 4. Data Preprocessing for Clustering Algorithms

Our raw patient data contains both numerical and categorical information. For clustering algorithms to work effectively, all features must be in a numerical format and on a comparable scale. As a Healthcare Administrator, I know that accurate data preparation is paramount for reliable analytical insights. I will transform categorical variables using one-hot encoding and scale numerical features to prevent features with larger values from dominating the distance calculations. This process ensures that patient similarities are measured fairly across all attributes.

For scaling numerical features, we typically use standardization, where each feature's values are transformed such that they have a mean of 0 and a standard deviation of 1. The formula for standardization for a data point $x$ in a feature with mean $\mu$ and standard deviation $\sigma$ is:
$$ x_{\text{scaled}} = \frac{x - \mu}{\sigma} $$
For categorical features, one-hot encoding converts each category into a new binary feature (0 or 1).
```
**Code Stubs (for `preprocessing.py` or `utils.py`):**
```python
def preprocess_patient_data(df):
    categorical_features = ['gender', 'insurance_type']
    numerical_features = ['age', 'medical_history_score', 'num_diagnoses',
                          'num_prescriptions', 'visit_frequency_per_year',
                          'chronic_condition_A_present', 'chronic_condition_B_present']

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ],
        remainder='drop'
    )
    preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])
    patient_features_processed = preprocessing_pipeline.fit_transform(df.drop('patient_id', axis=1))

    ohe_feature_names = preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)
    all_feature_names = numerical_features + list(ohe_feature_names)
    patient_processed_df = pd.DataFrame(patient_features_processed, columns=all_feature_names)
    return patient_processed_df, numerical_features, categorical_features
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation, including the LaTeX formula for standardization.
*   Call `preprocess_patient_data` with `st.session_state.patient_data` and store results in `st.session_state`.
*   `st.write("Shape of the processed patient data:", st.session_state.patient_processed_df.shape)`
*   `st.subheader("First 5 rows of the processed patient data:")`
*   `st.dataframe(st.session_state.patient_processed_df.head())`
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    After preprocessing, our data is now in a uniform, scaled numerical format, ready for clustering. The increase in the number of columns reflects the one-hot encoding of categorical variables. By standardizing numerical features, I've ensured that 'age' doesn't disproportionately influence clustering compared to 'num_prescriptions' simply because its values are numerically larger. This standardized representation allows the clustering algorithms to accurately identify patient similarities based on true underlying patterns, not just differences in scale.
    ```
    *   `st.markdown` for the explanation.

### **Section 5: Dimension Reduction for Visualizing Patient Relationships**

**Markdown Content:**
```markdown
## 5. Dimension Reduction for Visualizing Patient Relationships

As a Healthcare Administrator, directly visualizing relationships within our high-dimensional patient data (many features) is challenging. To gain an intuitive understanding of how patients might naturally group together, I'll use dimension reduction techniques. Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) will transform our data into a lower-dimensional space (2D or 3D) that can be easily plotted. This visualization step is crucial for spotting preliminary groupings and confirming whether our chosen clustering algorithms are likely to find meaningful cohorts.

### Principal Component Analysis (PCA)
PCA identifies the principal components, which are new variables that are linear combinations of the original variables and are orthogonal to each other. These components capture the maximum variance in the data. The algorithm involves:
1.  **Centering the data:** $X_{\text{centered}} = X - \text{mean}(X)$
2.  **Computing covariance matrix:** $C = \frac{1}{n-1} X_{\text{centered}}^T X_{\text{centered}}$
3.  **Performing eigendecomposition:** $C = V \Lambda V^T$, where $V$ contains eigenvectors (principal components) and $\Lambda$ contains eigenvalues (variance explained).
4.  **Selecting top $k$ eigenvectors:** To reduce dimensions.
5.  **Transforming data:** Projecting $X_{\text{centered}}$ onto the selected eigenvectors.

### t-Distributed Stochastic Neighbor Embedding (t-SNE)
t-SNE is particularly effective for visualizing high-dimensional data by giving each data point a location in a two- or three-dimensional map. It models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.
```
**Code Stubs (for `dimension_reduction.py` or `utils.py`):**
```python
def plot_dimension_reduction(data, method_name, n_components=2, perplexity=30, n_iter=1000):
    """
    Applies dimension reduction and plots the result.
    """
    if method_name == 'PCA':
        reducer = PCA(n_components=n_components, random_state=42)
        reduced_data = reducer.fit_transform(data)
        st.write(f"PCA explained variance ratio for {n_components} components: {reducer.explained_variance_ratio_.sum():.2f}")
    elif method_name == 't-SNE':
        reducer = TSNE(n_components=n_components, random_state=42, perplexity=perplexity, n_iter=n_iter, learning_rate='auto')
        reduced_data = reducer.fit_transform(data)
    else:
        raise ValueError("Method must be 'PCA' or 't-SNE'")

    reduced_df = pd.DataFrame(reduced_data, columns=[f'{method_name} Component {i+1}' for i in range(n_components)])

    fig = plt.figure(figsize=(10, 6))
    if n_components == 2:
        sns.scatterplot(data=reduced_df, x=f'{method_name} Component 1', y=f'{method_name} Component 2', alpha=0.7)
        plt.title(f'{method_name} 2D Projection of Patient Data')
    elif n_components == 3:
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(reduced_df[f'{method_name} Component 1'],
                   reduced_df[f'{method_name} Component 2'],
                   reduced_df[f'{method_name} Component 3'], alpha=0.7)
        ax.set_title(f'{method_name} 3D Projection of Patient Data')
        ax.set_xlabel(f'{method_name} Component 1')
        ax.set_ylabel(f'{method_name} Component 2')
        ax.set_zlabel(f'{method_name} Component 3')
    st.pyplot(fig)

    return reduced_data
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanations, including the LaTeX formulas for PCA.
*   Get user selections for method, n\_components, perplexity, n\_iter from `st.session_state`.
*   Call `plot_dimension_reduction` for PCA and t-SNE, storing `pca_reduced_data` and `tsne_reduced_data` in `st.session_state`.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The 2D plots from PCA and t-SNE give me an initial visual sense of how our patient population is structured. PCA, being linear, might show broader trends, while t-SNE, often better at preserving local neighborhood structures, could reveal more intricate, non-linear groupings. Seeing distinct "blobs" or concentrations of points in these visualizations suggests that our patient data likely contains discernible cohorts, which is a positive sign for the subsequent clustering analysis. The explained variance for PCA tells me how much information is retained, which helps in judging its representativeness.
    ```
    *   `st.markdown` for the explanation.

### **Section 6: Discovering Patient Cohorts with K-Means Clustering**

**Markdown Content:**
```markdown
## 6. Discovering Patient Cohorts with K-Means Clustering

Now that our data is preprocessed and we've had a preliminary visual inspection, it's time to apply clustering algorithms. As a Healthcare Administrator, my goal is to identify distinct patient cohorts whose needs and responses to treatments differ. K-Means clustering is an excellent starting point due to its simplicity and efficiency for numerical data. A crucial decision for K-Means is determining the optimal number of clusters, $k$. I'll use the Elbow Method and Silhouette Score to guide this choice, ensuring we find meaningful, compact, and well-separated groups.

The K-Means algorithm operates by:
1.  **Initializing $k$ centroids** randomly.
2.  **Iteratively assigning each data point** to the cluster whose centroid it is closest to. The distance $d(x_i, \mu_j)$ is typically Euclidean.
3.  **Updating the centroid** of each cluster to be the mean of all points assigned to it: $\mu_j = \text{mean of all points assigned to cluster } j$.
4.  **Repeating steps 2 and 3** until the centroids no longer change significantly (convergence) or a maximum number of iterations is reached.

The Silhouette Score for a data point $i$ is calculated as:
$$ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} $$
where $a(i)$ is the average distance from $i$ to other points in its own cluster (cohesion) and $b(i)$ is the minimum average distance from $i$ to points in any other cluster (separation). A higher Silhouette Score indicates better-defined clusters.
```
**Code Stubs (for `clustering.py` or `utils.py`):**
```python
def evaluate_kmeans_k(data, max_k):
    """
    Evaluates K-Means clustering for a range of k values using Elbow Method (SSE)
    and Silhouette Score. Returns the optimal k based on silhouette score.
    """
    sse = []
    silhouette_scores = []
    k_range = range(2, max_k + 1)

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(data)
        sse.append(kmeans.inertia_)
        score = silhouette_score(data, kmeans.labels_)
        silhouette_scores.append(score)

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    axes[0].plot(k_range, sse, marker='o')
    axes[0].set_title('Elbow Method for Optimal K')
    axes[0].set_xlabel('Number of clusters (k)')
    axes[0].set_ylabel('Sum of Squared Errors (SSE)')

    axes[1].plot(k_range, silhouette_scores, marker='o')
    axes[1].set_title('Silhouette Scores for Optimal K')
    axes[1].set_xlabel('Number of clusters (k)')
    axes[1].set_ylabel('Silhouette Score')
    plt.tight_layout()
    st.pyplot(fig)

    optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]
    st.info(f"Optimal K based on highest Silhouette Score: {optimal_k_silhouette}")
    return optimal_k_silhouette

def run_kmeans_clustering(data, k_clusters, pca_data, tsne_data):
    kmeans_model = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
    kmeans_labels = kmeans_model.fit_predict(data)

    # Visualize K-Means clusters on PCA and t-SNE projections
    pca_2d_df = pd.DataFrame(pca_data, columns=['PCA Component 1', 'PCA Component 2'])
    pca_2d_df['Cluster'] = kmeans_labels.astype(str)
    tsne_2d_df = pd.DataFrame(tsne_data, columns=['t-SNE Component 1', 't-SNE Component 2'])
    tsne_2d_df['Cluster'] = kmeans_labels.astype(str)

    fig_pca = plt.figure(figsize=(10, 6))
    sns.scatterplot(data=pca_2d_df, x='PCA Component 1', y='PCA Component 2', hue='Cluster', palette='viridis', alpha=0.8)
    plt.title(f'K-Means Clusters (k={k_clusters}) on PCA 2D Projection')
    st.pyplot(fig_pca)

    fig_tsne = plt.figure(figsize=(10, 6))
    sns.scatterplot(data=tsne_2d_df, x='t-SNE Component 1', y='t-SNE Component 2', hue='Cluster', palette='viridis', alpha=0.8)
    plt.title(f'K-Means Clusters (k={k_clusters}) on t-SNE 2D Projection')
    st.pyplot(fig_tsne)

    st.write(f"\nK-Means Cluster Sizes (k={k_clusters}):")
    st.dataframe(pd.Series(kmeans_labels).value_counts().sort_index().to_frame(name='Count'))
    return kmeans_labels
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation, including the LaTeX formula for Silhouette Score.
*   Call `evaluate_kmeans_k` using `st.session_state.patient_processed_df` to suggest optimal $k$. Update the default value of the $k$ slider in `st.session_state` if a new optimal $k$ is found.
*   Get the chosen $k$ from `st.session_state.num_clusters_input`.
*   Call `run_kmeans_clustering` and store `kmeans_labels` in `st.session_state`.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The Elbow Method and Silhouette Score plots provided insights into the most suitable number of clusters, $k$. A clear 'elbow' in the SSE plot, combined with a peak in the Silhouette Score, indicated an optimal `k` value. The scatter plots, colored by K-Means clusters on the PCA and t-SNE projections, visually confirmed that the algorithm successfully identified distinct patient groups. This visual validation is critical for me to trust that the mathematical groupings align with observable separations in the data. The cluster sizes give me an initial understanding of the distribution of patients across these new cohorts.
    ```
    *   `st.markdown` for the explanation.

### **Section 7: Exploring Alternative Patient Cohorts with Hierarchical Clustering**

**Markdown Content:**
```markdown
## 7. Exploring Alternative Patient Cohorts with Hierarchical Clustering

While K-Means is effective, its assumption of spherical clusters might not always capture the true underlying structure of our patient data. As a Healthcare Administrator, I want to explore alternative clustering methods to ensure we have a robust understanding of our patient population. Hierarchical clustering, specifically agglomerative clustering, offers a different perspective by building a tree-like structure (dendrogram) that shows how patient groups merge at different levels of similarity. This allows for a more flexible definition of clusters and can reveal nested relationships.

Agglomerative hierarchical clustering works "bottom-up":
1.  **Initialize:** Each data point is its own cluster.
2.  **Compute distance matrix:** Distances between all pairs of points.
3.  **Iteratively merge:** Find the closest pair of clusters and merge them into a new cluster. The "closest" is defined by a **linkage method**:
    *   **Single linkage:** $\min\{d(x, y) : x \in C_i, y \in C_j\}$ (minimum distance between points in two clusters).
    *   **Complete linkage:** $\max\{d(x, y) : x \in C_i, y \in C_j\}$ (maximum distance).
    *   **Average linkage:** $\text{mean}\{d(x, y) : x \in C_i, y \in C_j\}$ (average distance).
    *   **Ward linkage:** Minimizes the total within-cluster variance. This is often preferred as it tends to produce more compact, spherical clusters.
4.  **Repeat step 3** until all points belong to a single cluster or the desired number of clusters is reached. A dendrogram records the merge history.
```
**Code Stubs (for `clustering.py` or `utils.py`):**
```python
def run_hierarchical_clustering(data, k_clusters, pca_data, tsne_data, linkage_method='ward'):
    linked = linkage(data, method=linkage_method)

    fig_dendro = plt.figure(figsize=(18, 8))
    # Calculate threshold for coloring - simplified logic for streamlit based on optimal_k
    threshold_distance = 0
    if k_clusters > 1 and len(linked) >= k_clusters: # Ensure linked array has enough rows
        threshold_distance = linked[-(k_clusters-1), 2]

    dendrogram(linked,
               orientation='top',
               distance_sort='descending',
               show_leaf_counts=False,
               color_threshold=threshold_distance,
               above_threshold_color='grey'
              )
    plt.title(f'Hierarchical Clustering Dendrogram (cut for {k_clusters} clusters)')
    plt.xlabel('Patient Samples')
    plt.ylabel('Distance')
    plt.xticks([])
    st.pyplot(fig_dendro)

    hierarchical_model = AgglomerativeClustering(n_clusters=k_clusters, linkage=linkage_method)
    hierarchical_labels = hierarchical_model.fit_predict(data)

    pca_2d_df_hac = pd.DataFrame(pca_data, columns=['PCA Component 1', 'PCA Component 2'])
    pca_2d_df_hac['Cluster'] = hierarchical_labels.astype(str)
    tsne_2d_df_hac = pd.DataFrame(tsne_data, columns=['t-SNE Component 1', 't-SNE Component 2'])
    tsne_2d_df_hac['Cluster'] = hierarchical_labels.astype(str)

    fig_pca_hac = plt.figure(figsize=(10, 6))
    sns.scatterplot(data=pca_2d_df_hac, x='PCA Component 1', y='PCA Component 2', hue='Cluster', palette='viridis', alpha=0.8)
    plt.title(f'Hierarchical Clusters (k={k_clusters}) on PCA 2D Projection')
    st.pyplot(fig_pca_hac)

    fig_tsne_hac = plt.figure(figsize=(10, 6))
    sns.scatterplot(data=tsne_2d_df_hac, x='t-SNE Component 1', y='t-SNE Component 2', hue='Cluster', palette='viridis', alpha=0.8)
    plt.title(f'Hierarchical Clusters (k={k_clusters}) on t-SNE 2D Projection')
    st.pyplot(fig_tsne_hac)

    st.write(f"\nHierarchical Cluster Sizes (k={k_clusters}):")
    st.dataframe(pd.Series(hierarchical_labels).value_counts().sort_index().to_frame(name='Count'))
    return hierarchical_labels
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation.
*   Get the chosen $k$ and linkage method from `st.session_state`.
*   Call `run_hierarchical_clustering` and store `hierarchical_labels` in `st.session_state`.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The dendrogram provided a visual representation of how patient clusters are formed by merging individual patients and smaller groups. This tree structure is invaluable for understanding the granularity of relationships. Cutting the dendrogram at a specific 'distance' level revealed distinct clusters, which I chose to align with the `k` value from K-Means for direct comparison. The subsequent scatter plots showed how these hierarchical clusters manifest in the reduced-dimensional space. Observing similarities or differences between the K-Means and Hierarchical clustering visualizations helps me assess the robustness of the identified cohorts, informing which clustering solution might be more appropriate for our specific healthcare planning needs.
    ```
    *   `st.markdown` for the explanation.

### **Section 8: Evaluating and Comparing Clustering Solutions**

**Markdown Content:**
```markdown
## 8. Evaluating and Comparing Clustering Solutions

As a Healthcare Administrator, I can't solely rely on visual inspection to determine the best patient cohorts. I need quantitative metrics to evaluate the quality of our clustering solutions and to compare the results from K-Means and Hierarchical clustering. This will help me choose the most reliable grouping for developing targeted interventions. We'll use the Silhouette Score to assess how well-defined and separated the clusters are for each algorithm, and the Adjusted Rand Index (ARI) to compare the agreement between the two different clustering results.

### Silhouette Score
The Silhouette Score, as introduced previously, quantifies how similar an object is to its own cluster compared to other clusters. A higher score signifies better-defined clusters.

### Adjusted Rand Index (ARI)
The Adjusted Rand Index (ARI) is a measure of the similarity between two data clusterings. Unlike the raw Rand Index, ARI adjusts for chance, meaning a random assignment of clusters will yield an ARI close to 0, and perfect agreement will yield an ARI of 1. Negative values indicate worse-than-random agreement. This is particularly useful when comparing two different clustering algorithms applied to the same dataset.
$$ \text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]} $$
where $\text{RI}$ is the Rand Index, and $E[\text{RI}]$ is the expected Rand Index under the null hypothesis of random clustering. $\max(\text{RI})$ is the maximum possible value of the Rand Index.
```
**Code Stubs (for `evaluation.py` or `utils.py`):**
```python
def evaluate_clustering_solutions(processed_data, kmeans_labels, hierarchical_labels, k_clusters):
    kmeans_silhouette = silhouette_score(processed_data, kmeans_labels)
    st.write(f"K-Means Silhouette Score: {kmeans_silhouette:.3f}")

    hierarchical_silhouette = silhouette_score(processed_data, hierarchical_labels)
    st.write(f"Hierarchical Clustering Silhouette Score: {hierarchical_silhouette:.3f}")

    ari_score = adjusted_rand_score(kmeans_labels, hierarchical_labels)
    st.write(f"Adjusted Rand Index (ARI) between K-Means and Hierarchical Clustering: {ari_score:.3f}")

    kmeans_sample_silhouette_values = silhouette_samples(processed_data, kmeans_labels)

    fig_silhouette_plot = plt.figure(figsize=(10, 7))
    y_lower = 10
    for i in range(k_clusters):
        ith_cluster_silhouette_values = \
            kmeans_sample_silhouette_values[kmeans_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / k_clusters)
        plt.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    plt.axvline(x=kmeans_silhouette, color="red", linestyle="--", label=f'Avg Silhouette Score: {kmeans_silhouette:.3f}')
    plt.title("Silhouette plot for K-Means Clustering")
    plt.xlabel("Silhouette coefficient values")
    plt.ylabel("Cluster label")
    plt.legend()
    plt.yticks([])
    plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    st.pyplot(fig_silhouette_plot)
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation, including the LaTeX formula for ARI.
*   Call `evaluate_clustering_solutions` using `st.session_state.patient_processed_df`, `st.session_state.kmeans_labels`, `st.session_state.hierarchical_labels`, and `st.session_state.num_clusters_input`.
*   `st.write` for the numerical scores.
*   `st.pyplot` for the individual Silhouette plot.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The Silhouette Scores quantified the compactness and separation of the clusters from both K-Means and Hierarchical algorithms. A higher score for one method indicates it formed more distinct and well-defined groups. The individual Silhouette plot for K-Means provided a deeper look, showing how well each patient fits into their assigned cluster, and highlighting any clusters with particularly low scores, which might suggest misclassified patients or fuzzy boundaries. The Adjusted Rand Index (ARI) provided a direct measure of agreement between the two different clustering solutions. If ARI is high, it means both algorithms are largely agreeing on the patient groupings, which boosts my confidence in the discovered cohorts. If it's low, it suggests the choice of algorithm significantly impacts the resulting patient segmentation, requiring further investigation to understand why. This quantitative evaluation helps me make an informed decision about which clustering result is most reliable for our strategic planning.
    ```
    *   `st.markdown` for the explanation.

### **Section 9: Characterizing Patient Cohorts for Actionable Insights**

**Markdown Content:**
```markdown
## 9. Characterizing Patient Cohorts for Actionable Insights

To move from abstract clusters to actionable healthcare strategies, I, as a Healthcare Administrator, need to understand the defining characteristics of each patient cohort. This involves analyzing the average values of key patient attributes within each cluster. By creating "profile charts," I can clearly visualize what makes each group unique â€“ are they typically older, have more chronic conditions, visit frequently, or have a specific insurance type? This deep dive into cohort profiles is essential for developing highly targeted interventions that address the specific needs of each patient segment.
```
**Code Stubs (for `characterization.py` or `utils.py`):**
```python
def plot_cluster_profiles(df_with_clusters, cluster_column, numerical_features, categorical_features, optimal_k):
    """
    Generates profile charts for each cluster based on original (unscaled) features.
    """
    st.subheader(f"--- Profiling Clusters for {cluster_column.replace('_', ' ').title()} ---")

    cluster_means_numerical = df_with_clusters.groupby(cluster_column)[numerical_features].mean()
    categorical_proportions = {}
    for col in categorical_features:
        categorical_proportions[col] = df_with_clusters.groupby(cluster_column)[col].value_counts(normalize=True).unstack(fill_value=0)

    # Plotting numerical feature profiles
    num_cols_to_plot = len(numerical_features)
    n_rows_num = (num_cols_to_plot + 2) // 3
    fig, axes = plt.subplots(n_rows_num, 3, figsize=(18, n_rows_num * 5))
    axes = axes.flatten()

    for i, feature in enumerate(numerical_features):
        cluster_means_numerical[feature].plot(kind='bar', ax=axes[i], title=f'Mean {feature.replace("_", " ").title()} by Cohort', rot=45, color=sns.color_palette("tab10", optimal_k))
        axes[i].set_ylabel(f'Mean {feature.replace("_", " ").title()}')
        axes[i].set_xlabel('Cohort ID')

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    fig.suptitle(f'Numerical Feature Profiles by {cluster_column.replace("_", " ").title()}', fontsize=16)
    st.pyplot(fig)

    # Plotting categorical feature profiles
    cat_cols_to_plot = len(categorical_features)
    n_rows_cat = (cat_cols_to_plot + 1) // 2
    fig_cat, axes_cat = plt.subplots(n_rows_cat, 2, figsize=(15, n_rows_cat * 5))
    axes_cat = axes_cat.flatten()

    for i, feature in enumerate(categorical_features):
        categorical_proportions[feature].plot(kind='bar', ax=axes_cat[i], stacked=True, title=f'Proportion of {feature.replace("_", " ").title()} by Cohort', rot=45, cmap='viridis')
        axes_cat[i].set_ylabel('Proportion')
        axes_cat[i].set_xlabel('Cohort ID')

    for j in range(i + 1, len(axes_cat)):
        fig_cat.delaxes(axes_cat[j])

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    fig_cat.suptitle(f'Categorical Feature Profiles by {cluster_column.replace("_", " ").title()}', fontsize=16)
    st.pyplot(fig_cat)
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation.
*   Select the `cluster_column` (e.g., `'kmeans_cluster'` or `'hierarchical_cluster'`) based on user choice or highest Silhouette Score.
*   Create a temporary DataFrame by joining `st.session_state.patient_data` with the chosen cluster labels.
*   Call `plot_cluster_profiles` with this DataFrame and `st.session_state.numerical_features`, `st.session_state.categorical_features`, and `st.session_state.num_clusters_input`.
*   `st.pyplot` for the generated plots.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The profile charts vividly illustrate the distinguishing characteristics of each patient cohort. For instance, I can immediately see if Cohort 0 is predominantly older patients with multiple chronic conditions, while Cohort 1 consists of younger patients with fewer prescriptions. These visual summaries transform raw data into clear, digestible insights. By comparing the mean values and proportions of features across clusters, I can confidently articulate what defines each patient group. This detailed characterization forms the backbone of any personalized intervention strategy, allowing us to tailor care based on the specific needs of each identified segment of our patient population.
    ```
    *   `st.markdown` for the explanation.

### **Section 10: Generating a Patient Cohort Report: Actionable Strategies**

**Markdown Content:**
```markdown
## 10. Generating a Patient Cohort Report: Actionable Strategies

As a Healthcare Administrator, the ultimate goal of this analysis is to translate data insights into practical, actionable strategies for improving patient care and resource allocation. Based on the detailed profiles of our identified patient cohorts, I can now develop specific recommendations. This "Patient Cohort Report" will summarize the key characteristics of each cluster and outline potential personalized intervention strategies. This deliverable is critical for moving beyond a one-size-fits-all approach and for optimizing our healthcare services.

We will select the clustering solution (e.g., K-Means) that demonstrated better quality based on the Silhouette Scores and agreement with ARI. For this report, let's assume K-Means provided the most robust and interpretable clusters.
```
**Code Stubs (for `report_generation.py` or `utils.py`):**
```python
def generate_cohort_report(df_with_clusters, processed_data_for_silhouette, cluster_column, numerical_features, categorical_features):
    """
    Generates a summary report for each patient cohort with actionable intervention strategies.
    """
    num_clusters = df_with_clusters[cluster_column].nunique()
    report_sections = []

    report_sections.append("# Patient Cohort Report: Personalized Intervention Strategies\n")
    report_sections.append("This report outlines distinct patient cohorts identified through unsupervised clustering, along with tailored recommendations for improving care pathways and resource allocation.\n")
    report_sections.append(f"**Total Patients Analyzed:** {len(df_with_clusters)}  \n")
    report_sections.append(f"**Clustering Method Used:** {cluster_column.replace('_', ' ').title()} (k={num_clusters})  \n")

    current_silhouette_score = silhouette_score(processed_data_for_silhouette, df_with_clusters[cluster_column])
    report_sections.append(f"**Overall Silhouette Score:** {current_silhouette_score:.3f}\n")

    cluster_summary_numerical = df_with_clusters.groupby(cluster_column)[numerical_features].mean()
    cluster_summary_categorical_mode = {}
    for col in categorical_features:
        prop_df = df_with_clusters.groupby(cluster_column)[col].value_counts(normalize=True).unstack(fill_value=0)
        cluster_summary_categorical_mode[col] = prop_df.idxmax(axis=1)

    for i in range(num_clusters):
        report_sections.append(f"\n## Cohort {i}:\n")
        report_sections.append(f"**Number of Patients:** {len(df_with_clusters[df_with_clusters[cluster_column] == i])}\n")

        report_sections.append("\n### Key Characteristics:\n")

        report_sections.append("**Numerical Features (Mean):**\n")
        for feature in numerical_features:
            report_sections.append(f"- **{feature.replace('_', ' ').title()}:** {cluster_summary_numerical.loc[i, feature]:.2f}\n")

        report_sections.append("\n**Categorical Features (Most Prevalent):**\n")
        for feature in categorical_features:
            report_sections.append(f"- **{feature.replace('_', ' ').title()}:** {cluster_summary_categorical_mode[feature].loc[i]}\n")

        report_sections.append("\n### Potential Personalized Intervention Strategies:\n")

        age_mean = cluster_summary_numerical.loc[i, 'age']
        medical_score_mean = cluster_summary_numerical.loc[i, 'medical_history_score']
        num_prescriptions_mean = cluster_summary_numerical.loc[i, 'num_prescriptions']
        visit_freq_mean = cluster_summary_numerical.loc[i, 'visit_frequency_per_year']

        cc_a_prop = df_with_clusters[df_with_clusters[cluster_column] == i]['chronic_condition_A_present'].mean()
        cc_b_prop = df_with_clusters[df_with_clusters[cluster_column] == i]['chronic_condition_B_present'].mean()

        if age_mean < 45 and medical_score_mean < 4 and num_prescriptions_mean < 3:
            report_sections.append("- **Focus on preventative care & health education:** Younger, healthier patients could benefit from programs promoting long-term wellness, healthy lifestyle choices, and early screening for potential risks (e.g., vaccination drives, healthy eating workshops).\n")
            report_sections.append("- **Digital engagement:** Utilize mobile apps or online portals for appointment reminders, health resources, and telehealth consultations, given their likely tech-savviness.\n")
        elif age_mean > 60 and (cc_a_prop > 0.6 or cc_b_prop > 0.6) and num_prescriptions_mean > 5:
            report_sections.append("- **Comprehensive chronic disease management:** Older patients with multiple chronic conditions and prescriptions require coordinated care, medication reconciliation, and support for managing complex conditions. Implement disease-specific management programs.\n")
            report_sections.append("- **Geriatric assessments & fall prevention programs:** Tailor services to address common issues in elderly populations, improving safety and quality of life.\n")
            report_sections.append("- **Home health support & community outreach:** Explore options for in-home care or support groups to reduce hospital readmission rates and provide social support.\n")
        elif visit_freq_mean > 7 and medical_score_mean > 6:
            report_sections.append("- **Care coordination for high-utilizers:** Patients with high visit frequency and complex medical histories need robust care coordination to avoid fragmented care and optimize treatment plans. Assign dedicated care navigators.\n")
            report_sections.append("- **Specialized clinics or diagnostic pathways:** Offer dedicated support for their complex needs, potentially reducing unnecessary visits by streamlining diagnostics.\n")
        else:
            report_sections.append("- **Standardized care pathways with personalization options:** For general cohorts, focus on efficient, evidence-based care while offering flexibility for individual needs identified through regular screenings.\n")
            report_sections.append("- **Patient satisfaction surveys:** Regularly collect feedback to identify emerging needs, improve service delivery, and enhance patient experience across all touchpoints.\n")

    return "".join(report_sections)
```
**Streamlit Implementation:**
*   Display `st.markdown` for the section title and explanation.
*   Generate a full DataFrame by joining `st.session_state.patient_data` with the chosen cluster labels.
*   Call `generate_cohort_report` with this DataFrame, `st.session_state.patient_processed_df`, the chosen `cluster_column`, and feature lists.
*   `st.markdown` to display the generated report content.
*   **Explanation:**
    ```markdown
    ### Explanation of Execution
    The Patient Cohort Report provides a concise, yet comprehensive, summary of our patient population, broken down into distinct, actionable segments. For each cohort, the report details average attributes and suggests tailored intervention strategies. As a Healthcare Administrator, this is my key deliverable. For example, if Cohort 0 is characterized by young age and low medical history scores, the report recommends preventative care programs. If Cohort 1 consists of older patients with many prescriptions, it suggests comprehensive chronic disease management. This report serves as a strategic document, guiding our organization in allocating resources more effectively, designing targeted health campaigns, and ultimately, delivering more personalized and effective care to each patient. The explicit recommendations ensure that the data-driven insights translate directly into tangible improvements in patient outcomes and operational efficiency.
    ```
    *   `st.markdown` for the explanation.

---
